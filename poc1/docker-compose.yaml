services:
  mlflow_db:
    image: postgres:13
    container_name: mlflow_db
    env_file:
      - .env
    ports:
      - "5432:5432"
    networks:
      - ML_Shared_Network
    volumes:
      - postgres_data:/var/lib/postgresql/data
    command:
      ["postgres", "-c", "listen_addresses=*"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  mlflow-s3:
    image: minio/minio
    container_name: mlflow-s3
    env_file:
      - .env
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_API_CORS_ALLOW_ORIGIN=*
    command: server /data --console-address ":9001"
    networks:
      ML_Shared_Network:
        aliases:
          - minio.local
          - s3.local
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  mlflow_server:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow_server
    #  NO exponer puerto directamente, Nginx lo manejarÃ¡
    expose:
      - "5000"
    env_file:
      - .env
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:password@mlflow_db:5432/mlflowdb
    networks:
      - ML_Shared_Network
    command: >
      mlflow server 
      --default-artifact-root s3://mlflow/
      --host 0.0.0.0
      --port 5000
    depends_on:
      mlflow_db:
        condition: service_healthy
      mlflow-s3:
        condition: service_healthy
    restart: unless-stopped

 #  NGINX como proxy
  mlflow_proxy:
    image: nginx:alpine
    container_name: mlflow_proxy
    ports:
      - "5000:5000"  # Exponemos Nginx, no MLflow directamente
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - ML_Shared_Network
    depends_on:
      - mlflow_server
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5000/health || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 10s

  ml_workspace:
    build:
      context: .
      dockerfile: Dockerfile.ml 
    container_name: ml_workspace
    volumes:
      - .:/app
    env_file:
      - .env
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow_proxy:5000
    networks:
      - ML_Shared_Network
    depends_on:
      mlflow_proxy:
        condition: service_healthy
  
  ml_api_service:
    build:
      context: .
      dockerfile: Dockerfile.apiml
    container_name: ml_api_service
    ports:
      - "5001:5001"
    env_file: 
      - .env
    environment:
      # Reutilizamos las variables del .env para asegurar que MLflow las vea
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI}
      - MLFLOW_S3_ENDPOINT_URL=${MLFLOW_S3_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - MLFLOW_S3_IGNORE_TLS=true
    networks:
      - ML_Shared_Network
    restart: on-failure
    depends_on:
      mlflow_proxy:
        condition: service_healthy

networks:
  ML_Shared_Network:
    name: mlflow_network
    driver: bridge

volumes:
  postgres_data:
  minio_data:


